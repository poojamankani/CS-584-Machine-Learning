{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_input(filename):\n",
    "    input_data=[];\n",
    "    f=open(filename);\n",
    "    for line in f:\n",
    "        input_data.append(line.split());\n",
    "    return input_data;\n",
    "\n",
    "def create_feature_matrix(input_data):\n",
    "    x_list=[a[0:len(input_data[0])-1] for a in input_data];\n",
    "    x=np.matrix(x_list);\n",
    "    #x=np.transpose(x)\n",
    "    return x.astype(np.float);\n",
    "\n",
    "def create_y_matrix(input_data):\n",
    "    y_list=[a[len(input_data[0])-1] for a in input_data];\n",
    "    y=np.matrix(y_list);\n",
    "    y=np.transpose(y)\n",
    "    return y.astype(np.float);\n",
    "\n",
    "def mean_square_error(predicted_y, training_data_y):\n",
    "    difference_y_num=np.empty([predicted_y.shape[0], predicted_y.shape[1]]);\n",
    "    for i in range(predicted_y.shape[0]):\n",
    "        for j in range(predicted_y.shape[1]):\n",
    "            difference_y_num[i][j]=predicted_y[i][j]-training_data_y[i][j];\n",
    "            difference_y_num[i][j]=(difference_y_num[i][j]*difference_y_num[i][j])\n",
    "    difference_y=difference_y_num.sum()\n",
    "    MSE=(difference_y)/len(predicted_y);\n",
    "    return MSE;\n",
    "\n",
    "def predict(testing_data_x, theta, degree):\n",
    "    poly = PolynomialFeatures(degree);\n",
    "    Z=poly.fit_transform(testing_data_x);\n",
    "    predicted_y=np.dot(Z, theta);\n",
    "    return predicted_y;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(train_x, train_y, learning_rate, poly_degree):\n",
    "    poly = PolynomialFeatures(poly_degree);\n",
    "    Z=poly.fit_transform(train_x);\n",
    "    theta=np.ones((Z.shape[1],1));\n",
    "    for i in range(1000):\n",
    "        predicted_y=np.dot(Z, theta);\n",
    "        difference=predicted_y-train_y;\n",
    "        #print difference\n",
    "        #diff_loss=np.sum(difference**2)/ (2*train_x.shape[0]);\n",
    "        gradient=np.dot(np.transpose(Z), difference)/train_x.shape[0];\n",
    "        #gradient=np.dot(Z, difference\n",
    "        theta=theta-(learning_rate*gradient);\n",
    "    return theta;\n",
    "    \n",
    "    \n",
    "def do_cross_validation(data_x, data_y, degree,  n_folds, learning_rate):\n",
    "    cv = KFold(len(data_y), n_folds)\n",
    "    error_mean = []\n",
    "    error_mean_train = []\n",
    "    i=1;\n",
    "    for train_idx, test_idx in cv:\n",
    "        theta=gradient(data_x[train_idx], data_y[train_idx], learning_rate, degree)\n",
    "        predicted_y_train=predict(data_x[train_idx], theta, degree)\n",
    "        MSE_train=mean_square_error(predicted_y_train, data_y[train_idx]);\n",
    "        #print theta\n",
    "        predicted_y=predict(data_x[test_idx], theta, degree)\n",
    "        #print predicted_y\n",
    "        #print data_y[test_idx]\n",
    "        MSE=mean_square_error(predicted_y, data_y[test_idx]);\n",
    "        print \"Testing Error(Custom Model) For fold: %d MSE = %f\" %(i, MSE)\n",
    "        print \"Training Error For fold: %d MSE = %f\" %(i, MSE_train)\n",
    "        error_mean.append(MSE);\n",
    "        error_mean_train.append(MSE_train);\n",
    "        i=i+1;\n",
    "    avg_custom=np.mean(error_mean);\n",
    "    avg_custom_train=np.mean(error_mean_train);\n",
    "    return avg_custom, avg_custom_train;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_gradient_iterative(filename, fold, degree, learning_rate):\n",
    "    input_data=read_input(filename)\n",
    "    data_x=create_feature_matrix(input_data)\n",
    "    data_y=create_y_matrix(input_data)\n",
    "    #print \"after y\"\n",
    "    avg_custom, avg_custom_train=do_cross_validation(data_x, data_y, degree, fold, learning_rate)\n",
    "    print \"Testing Error (Custom Model) Average MSE: %f\" %(avg_custom)\n",
    "    print \"Training Error Average MSE: %f\" %(avg_custom_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error(Custom Model) For fold: 1 MSE = 0.268468\n",
      "Training Error For fold: 1 MSE = 0.258976\n",
      "Testing Error(Custom Model) For fold: 2 MSE = 0.264439\n",
      "Training Error For fold: 2 MSE = 0.259578\n",
      "Testing Error(Custom Model) For fold: 3 MSE = 0.235111\n",
      "Training Error For fold: 3 MSE = 0.262601\n",
      "Testing Error(Custom Model) For fold: 4 MSE = 0.254510\n",
      "Training Error For fold: 4 MSE = 0.260823\n",
      "Testing Error(Custom Model) For fold: 5 MSE = 0.272545\n",
      "Training Error For fold: 5 MSE = 0.258215\n",
      "Testing Error(Custom Model) For fold: 6 MSE = 0.291181\n",
      "Training Error For fold: 6 MSE = 0.257053\n",
      "Testing Error(Custom Model) For fold: 7 MSE = 0.271902\n",
      "Training Error For fold: 7 MSE = 0.259030\n",
      "Testing Error(Custom Model) For fold: 8 MSE = 0.213760\n",
      "Training Error For fold: 8 MSE = 0.264496\n",
      "Testing Error(Custom Model) For fold: 9 MSE = 0.286373\n",
      "Training Error For fold: 9 MSE = 0.257448\n",
      "Testing Error(Custom Model) For fold: 10 MSE = 0.269508\n",
      "Training Error For fold: 10 MSE = 0.259127\n",
      "Testing Error (Custom Model) Average MSE: 0.262780\n",
      "Training Error Average MSE: 0.259735\n"
     ]
    }
   ],
   "source": [
    "run_gradient_iterative(\"mvar-set1.dat\", 10, 3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error(Custom Model) For fold: 1 MSE = 0.012119\n",
      "Training Error For fold: 1 MSE = 0.012962\n",
      "Testing Error(Custom Model) For fold: 2 MSE = 0.012920\n",
      "Training Error For fold: 2 MSE = 0.013030\n",
      "Testing Error(Custom Model) For fold: 3 MSE = 0.014040\n",
      "Training Error For fold: 3 MSE = 0.012979\n",
      "Testing Error(Custom Model) For fold: 4 MSE = 0.014028\n",
      "Training Error For fold: 4 MSE = 0.012839\n",
      "Testing Error(Custom Model) For fold: 5 MSE = 0.012428\n",
      "Training Error For fold: 5 MSE = 0.012898\n",
      "Testing Error(Custom Model) For fold: 6 MSE = 0.011890\n",
      "Training Error For fold: 6 MSE = 0.012875\n",
      "Testing Error(Custom Model) For fold: 7 MSE = 0.013048\n",
      "Training Error For fold: 7 MSE = 0.012975\n",
      "Testing Error(Custom Model) For fold: 8 MSE = 0.012210\n",
      "Training Error For fold: 8 MSE = 0.012899\n",
      "Testing Error(Custom Model) For fold: 9 MSE = 0.014704\n",
      "Training Error For fold: 9 MSE = 0.012946\n",
      "Testing Error(Custom Model) For fold: 10 MSE = 0.013499\n",
      "Training Error For fold: 10 MSE = 0.013023\n",
      "Testing Error (Custom Model) Average MSE: 0.013089\n",
      "Training Error Average MSE: 0.012943\n"
     ]
    }
   ],
   "source": [
    "run_gradient_iterative(\"mvar-set2.dat\", 10, 3, 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Error(Custom Model) For fold: 1 MSE = 0.248989\n",
      "Training Error For fold: 1 MSE = 0.253424\n",
      "Testing Error(Custom Model) For fold: 2 MSE = 0.253653\n",
      "Training Error For fold: 2 MSE = 0.252877\n",
      "Testing Error(Custom Model) For fold: 3 MSE = 0.254279\n",
      "Training Error For fold: 3 MSE = 0.252797\n",
      "Testing Error(Custom Model) For fold: 4 MSE = 0.254499\n",
      "Training Error For fold: 4 MSE = 0.252862\n",
      "Testing Error(Custom Model) For fold: 5 MSE = 0.256500\n",
      "Training Error For fold: 5 MSE = 0.252603\n",
      "Testing Error(Custom Model) For fold: 6 MSE = 0.252571\n",
      "Training Error For fold: 6 MSE = 0.252911\n",
      "Testing Error(Custom Model) For fold: 7 MSE = 0.252700\n",
      "Training Error For fold: 7 MSE = 0.253019\n",
      "Testing Error(Custom Model) For fold: 8 MSE = 0.250606\n",
      "Training Error For fold: 8 MSE = 0.253246\n",
      "Testing Error(Custom Model) For fold: 9 MSE = 0.252032\n",
      "Training Error For fold: 9 MSE = 0.253124\n",
      "Testing Error(Custom Model) For fold: 10 MSE = 0.255158\n",
      "Training Error For fold: 10 MSE = 0.252820\n",
      "Testing Error (Custom Model) Average MSE: 0.253099\n",
      "Training Error Average MSE: 0.252968\n"
     ]
    }
   ],
   "source": [
    "run_gradient_iterative(\"mvar-set3.dat\", 10, 2, 0.01)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
