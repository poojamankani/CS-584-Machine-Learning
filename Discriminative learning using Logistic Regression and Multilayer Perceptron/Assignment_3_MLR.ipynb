{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, precision_score,recall_score,roc_curve,precision_recall_curve, auc,f1_score, confusion_matrix\n",
    "import operator\n",
    "from scipy.optimize import fmin_bfgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_mldata\n",
    "import numpy as np\n",
    "import os\n",
    "from pylab import *\n",
    "mnist = fetch_mldata('MNIST original')\n",
    "mnist.data.shape\n",
    "mnist.target.shape\n",
    "np.unique(mnist.target)\n",
    "X_m, y_m = mnist.data / 255., mnist.target\n",
    "X_train, X_test = X_m[:50000], X_m[50000:]\n",
    "y_train, y_test = y_m[:50000], y_m[50000:]\n",
    "X_train.shape\n",
    "y_train.shape\n",
    "size=len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3147L, 784L)\n",
      "(3147L,)\n",
      "set([1, 2, 3])\n",
      "(13623L, 784L)\n",
      "(13623L,)\n",
      "set([1, 2, 3])\n"
     ]
    }
   ],
   "source": [
    "X_train_m= []\n",
    "Y_train_m=[]\n",
    "X_test_m= []\n",
    "Y_test_m=[]\n",
    "for i in range(y_test.shape[0]):\n",
    "    if y_test[i] == 0.0:\n",
    "        X_train_m.append(X_test[i])\n",
    "        Y_train_m.append(1);\n",
    "    elif y_test[i] == 1.0:\n",
    "        X_train_m.append(X_test[i])\n",
    "        Y_train_m.append(2);\n",
    "    elif y_test[i] == 2.0:\n",
    "        X_train_m.append(X_test[i])\n",
    "        Y_train_m.append(3);\n",
    "X_train_m=np.array(X_train_m)\n",
    "Y_train_m=np.array(Y_train_m)\n",
    "print X_train_m.shape\n",
    "print Y_train_m.shape\n",
    "print set(Y_train_m)\n",
    "for i in range(y_train.shape[0]):\n",
    "    if y_train[i] == 0.0:\n",
    "        X_test_m.append(X_test[i])\n",
    "        Y_test_m.append(1);\n",
    "    elif y_train[i] == 1.0:\n",
    "        X_test_m.append(X_test[i])\n",
    "        Y_test_m.append(2);\n",
    "    elif y_train[i] == 2.0:\n",
    "        X_test_m.append(X_test[i])\n",
    "        Y_test_m.append(3);\n",
    "X_test_m=np.array(X_test_m[5000:30000])\n",
    "Y_test_m=np.array(Y_test_m[5000:30000])\n",
    "print X_test_m.shape\n",
    "print Y_test_m.shape\n",
    "print set(Y_test_m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_input(filename):\n",
    "    input_data=[];\n",
    "    f=open(filename);\n",
    "    #f.readline();\n",
    "    #f.readline();\n",
    "    for line in f:\n",
    "        input_data.append(line.rstrip('\\n').split(','));\n",
    "    return input_data;\n",
    "\n",
    "def create_feature_matrix(input_data):\n",
    "    x_list=[a[0:len(input_data[0])-1] for a in input_data];\n",
    "    x=np.array([[1]+a[0:len(input_data[0])-1] for a in input_data]);\n",
    "    #x=np.transpose(x)\n",
    "    x= x.astype(np.float);\n",
    "    return x\n",
    "\n",
    "def create_y_matrix(input_data):\n",
    "    y_list=[a[len(input_data[0])-1] for a in input_data];\n",
    "    y_list_1=[];\n",
    "    for i in y_list:\n",
    "        if(i=='Iris-setosa'):\n",
    "            y_list_1.append(1);\n",
    "        elif(i=='Iris-versicolor'):\n",
    "            y_list_1.append(2);\n",
    "        elif(i=='Iris-virginica'):\n",
    "            y_list_1.append(3);\n",
    "        \n",
    "            \n",
    "    y=np.array(y_list_1);\n",
    "    #y=np.transpose(y)\n",
    "    y= y.astype(np.float);\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150L, 5L)\n",
      "(150L,)\n"
     ]
    }
   ],
   "source": [
    "#input_data=read_input('winequality-red.csv')\n",
    "input_data=read_input('iris.dat')\n",
    "X=create_feature_matrix(input_data)\n",
    "Y=create_y_matrix(input_data)\n",
    "print X.shape\n",
    "print Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def l_theta(x,y, theta,c=0 ):\n",
    "    \"\"\"m=y.shape[0];\n",
    "    print theta.shape\n",
    "    print x.shape\n",
    "    print (np.dot(x,np.transpose(np.matrix(theta)))).shape\n",
    "    h_t=h_theta(np.transpose(np.matrix(theta))*x);\n",
    "    l_theta=np.sum((y*log(h_t))-((1-y)-log(1-h_t)));\n",
    "    l_theta=1.*l_theta/m;\n",
    "    return l_theta\"\"\"\n",
    "    theta = theta.reshape((x.shape[1], y.shape[1]))\n",
    "    y1=np.dot(x,theta);\n",
    "    y1=y1-y1.min(axis=1)[:,np.newaxis]\n",
    "    y1=np.exp(-y1);\n",
    "    y1=y1/y1.sum(axis=1)[:,np.newaxis]\n",
    "    y11=y1*y;\n",
    "    log_theta=np.sum(np.log(y11.sum(axis=1)))\n",
    "    y11=y11/y11.sum(axis=1)[:,np.newaxis]\n",
    "    y1=y1-y11;\n",
    "    log_theta=log_theta/-float(x.shape[0])\n",
    "    return log_theta;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gradient(x,y,theta, c=0.1):\n",
    "    theta = theta.reshape((x.shape[1], y.shape[1]))\n",
    "    y1=np.dot(x,theta);\n",
    "    y1=y1-y1.min(axis=1)[:,np.newaxis]\n",
    "    y1=np.exp(-y1);\n",
    "    y1=y1/y1.sum(axis=1)[:,np.newaxis]\n",
    "    y11=y1*y;\n",
    "    y11=y11/y11.sum(axis=1)[:,np.newaxis]\n",
    "    y1=y1-y11;\n",
    "    grad=np.dot(np.transpose(x), y1);\n",
    "    grad=grad/-float(x.shape[0])\n",
    "    \n",
    "    return grad.ravel();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(x_test, y_test, theta, classarray):\n",
    "    predicted_y1=[]\n",
    "    theta = theta.reshape((x_test.shape[1], y_test.shape[1]))\n",
    "    predicted_y=np.dot(x_test, theta)\n",
    "    predicted_y=predicted_y-predicted_y.min(axis=1)[:,np.newaxis]\n",
    "    predicted_y=np.exp(-predicted_y)\n",
    "    predicted_y=predicted_y/predicted_y.sum(axis=1)[:,np.newaxis]\n",
    "    #print predicted_y\n",
    "    predicted_y2 = np.zeros((x_test.shape[0], len(classarray)), dtype=np.float64)\n",
    "    predicted_y1=np.argmax(predicted_y, axis=1).squeeze()+1\n",
    "    for i, a in enumerate(classarray):\n",
    "        predicted_y2[predicted_y1==a, i]=1;\n",
    "    return predicted_y2;    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calc_theta(x,y, c =0):\n",
    "    def optimize_theta(theta):\n",
    "        return l_theta(x,y,theta,c)\n",
    "    \n",
    "    def optimize_grad(theta):\n",
    "        return gradient(x,y,theta,c)\n",
    "    \n",
    "    initial_theta=np.zeros((x.shape[1]* y.shape[1],1));\n",
    "    \n",
    "    return fmin_bfgs(optimize_theta, initial_theta, fprime=optimize_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def do_cross_validation(X, y,  classarray,n_folds, verbose=False):\n",
    "    \n",
    "    accuracies = []\n",
    "    precisions =[]\n",
    "    recalls=[]\n",
    "    f1_scores=[]\n",
    "    y_p=[]\n",
    "    i=1;\n",
    "    X1=X;\n",
    "        \n",
    "    Y1 = np.zeros((X1.shape[0], len(classarray)), dtype=np.float64)\n",
    "    for i, a in enumerate(classarray):\n",
    "        Y1[y==a, i]=1;\n",
    "    print Y1\n",
    "    cv = KFold(len(Y1), n_folds, shuffle=True, random_state=5)\n",
    "    \n",
    "    for train_idx, test_idx in cv:\n",
    "        theta=calc_theta(X1[train_idx],Y1[train_idx]);\n",
    "        #theta=theta[0].reshape((X1[train_idx].shape[1], Y1[train_idx].shape[1]))\n",
    "        print theta\n",
    "        predicted_y=predict(X1[test_idx], Y1[test_idx], theta, classarray);\n",
    "        #print Y1[test_idx];\n",
    "        #print predicted_y;\n",
    "        acc = accuracy_score(Y1[test_idx], predicted_y)\n",
    "        #c_matrix= confusion_matrix(Y1[test_idx], predicted_y)\n",
    "        precision=precision_score(Y1[test_idx], predicted_y, average=\"weighted\")\n",
    "        recall=recall_score(Y1[test_idx], predicted_y, average=\"weighted\")\n",
    "        f_score=f1_score(Y1[test_idx], predicted_y, average=\"weighted\")\n",
    "        if(verbose == True):\n",
    "            print('fold %d accuracy=%.4f, precision=%.4f, recall=%.4f, F-measure%.4f, confusion matrix=' % (i, acc, precision, recall, f_score));\n",
    "            #print c_matrix\n",
    "        i=i+1;\n",
    "        accuracies.append(acc)\n",
    "        precisions.append(precision)\n",
    "        recalls.append(recall)\n",
    "        f1_scores.append(f_score)\n",
    "        prredicted_y=[]\n",
    "    avg=np.mean(accuracies);\n",
    "    avg_precision=np.mean(precisions)\n",
    "    avg_recall=np.mean(recalls)\n",
    "    avg_f1_score=np.mean(f1_scores)\n",
    "    return avg, avg_precision, avg_recall, avg_f1_score;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000001\n",
      "         Iterations: 64\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 66\n",
      "[ -165.3917522  -1425.22485774  1590.61660995  -307.84929072   192.84046732\n",
      "   115.00882341  -804.50230232    54.2488825    750.2534198   1147.72165811\n",
      "  -453.83500944  -693.88664873   528.16505481   723.36514621 -1251.53020102]\n",
      "fold 2 accuracy=0.8667, precision=0.9048, recall=0.8667, F-measure0.8644, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.041271\n",
      "         Iterations: 63\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 65\n",
      "[ -2.28620785 -17.39174918  19.67795703  -4.31613941   1.03062965\n",
      "   3.28550977 -10.97051503   2.22319693   8.7473181   15.88590236\n",
      "  -3.56008494 -12.32581742   7.35087587   4.3067968  -11.65767267]\n",
      "fold 3 accuracy=1.0000, precision=1.0000, recall=1.0000, F-measure1.0000, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.044055\n",
      "         Iterations: 63\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 65\n",
      "[ -2.50232825 -20.02200396  22.52433221  -4.85315355   1.19698134\n",
      "   3.65617221 -11.83977847   2.58541045   9.25436802  17.17126266\n",
      "  -3.88051971 -13.29074294   8.03201464   5.10701381 -13.13902845]\n",
      "fold 4 accuracy=1.0000, precision=1.0000, recall=1.0000, F-measure1.0000, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.042119\n",
      "         Iterations: 64\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 66\n",
      "[ -2.31484875 -18.53466631  20.84951506  -4.33453393   0.79595366\n",
      "   3.53858028 -10.95297409   2.58958193   8.36339215  15.73360565\n",
      "  -3.17625193 -12.55735373   7.20334089   4.34613608 -11.54947697]\n",
      "fold 5 accuracy=1.0000, precision=1.0000, recall=1.0000, F-measure1.0000, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.039391\n",
      "         Iterations: 66\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 68\n",
      "[ -2.5460376  -20.33395288  22.87999048  -4.79245607   1.61217979\n",
      "   3.18027628 -12.23034787   3.00797816   9.22236971  18.0583341\n",
      "  -4.75296526 -13.30536884   8.18996847   4.51234293 -12.7023114 ]\n",
      "fold 6 accuracy=1.0000, precision=1.0000, recall=1.0000, F-measure1.0000, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.044041\n",
      "         Iterations: 64\n",
      "         Function evaluations: 66\n",
      "         Gradient evaluations: 66\n",
      "[ -2.33897646 -20.01632799  22.35530445  -4.60530601   1.07284053\n",
      "   3.53246548 -11.5081125    2.42721861   9.0808939   16.49967485\n",
      "  -3.55924231 -12.94043254   7.4675937    5.36709589 -12.83468959]\n",
      "fold 7 accuracy=1.0000, precision=1.0000, recall=1.0000, F-measure1.0000, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.039923\n",
      "         Iterations: 63\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 65\n",
      "[ -2.44250606 -19.73599965  22.17850571  -4.59173392   1.09944606\n",
      "   3.49228786 -11.55846957   2.61899214   8.93947743  16.66046564\n",
      "  -3.63385919 -13.02660645   7.66932397   4.63364523 -12.3029692 ]\n",
      "fold 8 accuracy=1.0000, precision=1.0000, recall=1.0000, F-measure1.0000, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000007\n",
      "         Iterations: 60\n",
      "         Function evaluations: 63\n",
      "         Gradient evaluations: 63\n",
      "[ -3338.5743137   -4275.70341331   7614.27772117  -6402.63956332\n",
      "  -4695.47290231  11098.11247085 -16167.8637776     245.93107282\n",
      "  15921.93272525  23398.44141807     96.16810893 -23494.60952568\n",
      "  10927.36840811   6195.41814577 -17122.78655704]\n",
      "fold 9 accuracy=0.9333, precision=0.9444, recall=0.9333, F-measure0.9316, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.042407\n",
      "         Iterations: 65\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 67\n",
      "[ -2.49284749 -23.6378789   26.13072639  -4.81934701   1.68717182\n",
      "   3.13217519 -12.50804181   3.10765575   9.40038607  17.44220569\n",
      "  -4.00115734 -13.44104835   8.09823823   4.97138161 -13.06961984]\n",
      "fold 10 accuracy=1.0000, precision=1.0000, recall=1.0000, F-measure1.0000, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.044063\n",
      "         Iterations: 63\n",
      "         Function evaluations: 65\n",
      "         Gradient evaluations: 65\n",
      "[ -2.42135508 -20.09815592  22.519511    -4.7576804    1.14657642\n",
      "   3.61110398 -11.92036349   2.61778068   9.30258281  17.38628378\n",
      "  -3.97841392 -13.40786987   7.9615259    5.15821303 -13.11973893]\n",
      "fold 11 accuracy=1.0000, precision=1.0000, recall=1.0000, F-measure1.0000, confusion matrix=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.98000000000000009,\n",
       " 0.98492063492063497,\n",
       " 0.98000000000000009,\n",
       " 0.97960461760461759)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_cross_validation(X, Y,[1,2,3],10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.039662\n",
      "         Iterations: 62\n",
      "         Function evaluations: 64\n",
      "         Gradient evaluations: 64\n",
      "[ -2.44482501 -20.18460599  22.629431    -4.6648963    1.1010832\n",
      "   3.5638131  -11.88180094   2.60127544   9.28052551  17.11865031\n",
      "  -3.83475439 -13.28389592   7.88103212   5.22011308 -13.1011452 ]\n",
      "Accuracy: 0.986667\n",
      "Precision: 0.986667\n",
      "Recall: 0.986667\n",
      "F1 Score: 0.986667\n"
     ]
    }
   ],
   "source": [
    "X1=X;\n",
    "Y1 = np.zeros((X1.shape[0], 3), dtype=np.float64)\n",
    "for i, a in enumerate([1,2,3]):\n",
    "    Y1[Y==a, i]=1;\n",
    "theta=calc_theta(X1,Y1);\n",
    "#theta=theta[0].reshape((X1[train_idx].shape[1], Y1[train_idx].shape[1]))\n",
    "print theta\n",
    "predicted_y=predict(X1, Y1, theta, [1,2,3]);\n",
    "#print Y1[test_idx];\n",
    "#print predicted_y;\n",
    "print \"Accuracy: %f\" %accuracy_score(Y1, predicted_y)\n",
    "print \"Precision: %f\" %precision_score(Y1, predicted_y, average=\"weighted\")\n",
    "print \"Recall: %f\" %recall_score(Y1, predicted_y, average=\"weighted\")\n",
    "print \"F1 Score: %f\" %f1_score(Y1, predicted_y, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " ..., \n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]\n",
      " [ 0.  0.  1.]]\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000014\n",
      "         Iterations: 74\n",
      "         Function evaluations: 76\n",
      "         Gradient evaluations: 76\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "fold 2 accuracy=0.9905, precision=0.9905, recall=0.9905, F-measure0.9905, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000023\n",
      "         Iterations: 61\n",
      "         Function evaluations: 62\n",
      "         Gradient evaluations: 62\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "fold 3 accuracy=0.9905, precision=0.9906, recall=0.9905, F-measure0.9905, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000011\n",
      "         Iterations: 72\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 73\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "fold 5 accuracy=0.9810, precision=0.9810, recall=0.9810, F-measure0.9809, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000017\n",
      "         Iterations: 67\n",
      "         Function evaluations: 68\n",
      "         Gradient evaluations: 68\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "fold 6 accuracy=0.9873, precision=0.9875, recall=0.9873, F-measure0.9873, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000013\n",
      "         Iterations: 68\n",
      "         Function evaluations: 69\n",
      "         Gradient evaluations: 69\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "fold 7 accuracy=0.9810, precision=0.9815, recall=0.9810, F-measure0.9810, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000009\n",
      "         Iterations: 72\n",
      "         Function evaluations: 73\n",
      "         Gradient evaluations: 73\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "fold 8 accuracy=0.9937, precision=0.9937, recall=0.9937, F-measure0.9936, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000010\n",
      "         Iterations: 73\n",
      "         Function evaluations: 74\n",
      "         Gradient evaluations: 74\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "fold 9 accuracy=0.9968, precision=0.9968, recall=0.9968, F-measure0.9968, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000013\n",
      "         Iterations: 66\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 67\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "fold 10 accuracy=0.9777, precision=0.9780, recall=0.9777, F-measure0.9776, confusion matrix=\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000011\n",
      "         Iterations: 66\n",
      "         Function evaluations: 67\n",
      "         Gradient evaluations: 67\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "fold 11 accuracy=0.9713, precision=0.9713, recall=0.9713, F-measure0.9713, confusion matrix=\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.98601455868971788,\n",
       " 0.98613362687732153,\n",
       " 0.98601455868971788,\n",
       " 0.98600496677486082)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "do_cross_validation(X_train_m, Y_train_m,[1,2,3],10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.000008\n",
      "         Iterations: 80\n",
      "         Function evaluations: 81\n",
      "         Gradient evaluations: 81\n",
      "[ 0.  0.  0. ...,  0.  0.  0.]\n",
      "Accuracy: 1.000000\n",
      "Precision: 1.000000\n",
      "Recall: 1.000000\n",
      "F1 Score: 1.000000\n"
     ]
    }
   ],
   "source": [
    "X1=X_train_m;\n",
    "Y1 = np.zeros((X1.shape[0], 3), dtype=np.float64)\n",
    "for i, a in enumerate([1,2,3]):\n",
    "    Y1[Y_train_m==a, i]=1;\n",
    "theta=calc_theta(X1,Y1);\n",
    "#theta=theta[0].reshape((X1[train_idx].shape[1], Y1[train_idx].shape[1]))\n",
    "print theta\n",
    "predicted_y=predict(X1, Y1, theta, [1,2,3]);\n",
    "#print Y1[test_idx];\n",
    "#print predicted_y;\n",
    "print \"Accuracy: %f\" %accuracy_score(Y1, predicted_y)\n",
    "print \"Precision: %f\" %precision_score(Y1, predicted_y, average=\"weighted\")\n",
    "print \"Recall: %f\" %recall_score(Y1, predicted_y, average=\"weighted\")\n",
    "print \"F1 Score: %f\" %f1_score(Y1, predicted_y, average=\"weighted\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
